<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VisMin</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VisMin: Visual Minimal-Change Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://rabiul.me/" target="_blank">Rabiul Awal</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://mila.quebec/en/saba-ahmadi" target="_blank">Saba Ahmadi</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://zhangle.netlify.app/" target="_blank">Le Zhang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.iro.umontreal.ca/~agrawal/" target="_blank">Aishwarya Agrawal</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Mila - Quebec AI Institute<br>Preprint</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="YOUR_PDF_HERE" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper (incoming)</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (incoming)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (incoming)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/example.png" id="tree" alt="Your image description" style="width:100%; height:auto;">
      <h2 class="subtitle has-text-centered">
        Overview of our VisMIn benchmark. ViMin consists of four types of minimal-changes -- object, attribute, count and spatial relation -- between two image-captions pairs. The evaluation task requires a model to predict the correct image-caption match given: 1) two images and one caption, 2) two captions and one image.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). To evaluate VLMs' fine-grained understanding, existing benchmarks primarily focus on evaluating VLMs' capability to distinguish between two very similar captions given an image. In this paper, our focus is on evaluating VLMs' capability to distinguish between two very similar images given a caption. To this end, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. Importantly, the image pair (as well as the caption pair) contains minimal changes, i.e., between the two images (as well as between the two captions), only one aspect changes at a time from among the following possible types of changes: object, attribute, count, and spatial relation. These four types of minimal changes are specifically designed to test the models' understanding of objects, attributes of objects (such as color, material, shape), counts of objects, and spatial relationships between objects. To curate our benchmark, we built an automatic pipeline using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. Furthermore, leveraging the automated nature of our data creation process, we generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model). Our findings show that both these models benefit significantly from fine-tuning on this data, as evident by marked improvements in fine-grained understanding across a wide range of benchmarks. Additionally, such fine-tuning improves CLIP's general image-text alignment capabilities too. All resources including the benchmark, the training data, and the finetuned model checkpoints will be released.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Overview pipeline-->
<section class="hero teaser">
  <div class="container is-max-desktop">
  <div class="hero-body">

    <h2 class="title is-3">Minimal-Change Image-Text Dataset Creation</h2>

    <div class=" is-light">

      <img src="static/images/pipeline.png" id="tree" alt="Your image description" style="width:100%; height:auto;">
    </div>

    <h2 class="subtitle has-text-centered">
      Our dataset creation pipeline includes three stages:
    </h2>
    <ol style="margin-top: 0; padding-left: 1.5em;">
      <li><strong>Image Synthesis:</strong> We develop methods for synthesizing images involving Objects & Attributes and Counting & Spatial Relations.</li>
      <li><strong>Automatic Filtering:</strong> An LLM generates questions and answers based on captions, and a VQA model predicts answers from images. Images are excluded if answers don't match.</li>
      <li><strong>Human Verification:</strong> Synthetic data undergoes a rigorous 4-steps human verification, and only examples passing all stages are included in the benchmark.</li>
    </ol>    

  </div>
</section>
<!-- End overview pipeline -->

<!-- Image ramndom -->

<!-- <div class="hero-body">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Random Image from VisMin</h2>
    <p>The VisMin dataset consists of four categories of minimal changes: object, attribute, count, and spatial relation. Below is a randomly sampled image from this dataset.</p>
  </div>
  <div class="container">
    <img src="static/images/VisMin-Random.svg" id="tree" alt="Your image description" style="width:100%; height:auto;">
  </div>
</div> -->
<!-- End image random -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/vismin_random_object.svg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Object.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/vismin_random_attribute.svg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Attribute.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/vismin_random_counting.svg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Counting.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/vismin_random_srelation.svg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Spatial Relationship.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- VisMin Benchmark -->
<section class="section is-small is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Training and Benchmark sets</h2>

    <p>This study aims to enhance fine-grained understanding in VLMs by creating training and benchmark sets. The training data is auto-filtered and scalable, while the benchmark data undergoes rigorous human verification for quality. The training data is sourced from VSR and the COCO 2017 training split, while the benchmark data comes from the COCO 2017 validation split. This ensures benchmark images are unseen during training. The training dataset has 64,392 samples, and the VisMin dataset has 2,084 samples. The study aimed for a balanced benchmark across categories, but attribute samples in the benchmark are relatively low due to limitations in suggested edits. The study provides an overview of the types of changes in the benchmark and detailed information on training set subcategories.</p>
    <div class="container"  style="display: flex; justify-content: center;">
      <img src="static/images/training_distribution.png" alt="VisMin Benchmark" style="width:50%; height:auto; margin-right: 2.5%;">
      <img src="static/images/VisMin_distribution.png" alt="VisMin Benchmark" style="width:50%; height:auto; margin-left: 2.5%;">
    </div>
  </div>
</section>

<!-- VisMin Results -->
<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Zero-shot Performance on the VisMin Benchmark</h2>

    <article class="message is-info">
      <div class="message-body">
        <strong>Setup.</strong> We evaluated a range of state-of-the-art Visual Language Models (VLMs) and Multimodal Large Language Models (MLLMs) including well-known foundational models like CLIP, SigLip, and emerging MLLMs like Idefics2 and GPT-4V. These evaluations covered image-text matching tasks, where models chose the correct image from two captions or the correct caption from two images, and adapted visual question answering formats for MLLMs, assessing alignment with captions across paired images.
      </div>
    </article>

    <div class="table-wrapper mb-5">
      <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
        <caption>Performance of foundational variants and MLLMs across categories on the VisMin Dataset. Columns 'I,' 'T,' and 'G' denote Image, Text, and Group scores from Winoground. 'AVG' denotes average across columns. Closed-source models were assessed using 400 random samples for object, attribute, and count tasks, and the full 294 samples for attributes, due to slow inference and time constraints.</caption>
        <thead class="center">
          <tr class="oddrow">
            <th></th>
            <th colspan="3">Object</th>
            <th colspan="3">Attribute</th>
            <th colspan="3">S. Relation</th>
            <th colspan="3">Count</th>
            <th rowspan="2">AVG</th>
          </tr>
          <tr style="border-bottom: 2px solid #aaa;">
            <th></th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
          </tr>
        </thead>
        <tbody class="center">
          <tr class="oddrow">
            <td style="text-align: left;">Random Chance</td>
            <td>25</td>
            <td>25</td>
            <td>16.67</td>
            <td>25</td>
            <td>25</td>
            <td>16.67</td>
            <td>25</td>
            <td>25</td>
            <td>16.67</td>
            <td>25</td>
            <td>25</td>
            <td>16.67</td>
            <td>22.22</td>
          </tr>
          <tr>
            <td style="text-align: left;">MTurk Human</td>
            <td>86.87</td>
            <td>95.50</td>
            <td>83.07</td>
            <td>82.31</td>
            <td>91.15</td>
            <td>76.87</td>
            <td>81.67</td>
            <td>92.76</td>
            <td>76.20</td>
            <td>88.96</td>
            <td>96.77</td>
            <td>86.41</td>
            <td>86.54</td>
          </tr>

          <tr class="family-header">
            <td colspan="14">Foundational VLMs</td>
          </tr>

          <tr>
            <td style="text-align: left;">CLIP (ViT-B/32)</td>
            <td>79.62</td>
            <td>77.89</td>
            <td>67.53</td>
            <td>72.11</td>
            <td>65.99</td>
            <td>55.1</td>
            <td>8.2</td>
            <td>4.34</td>
            <td>0.48</td>
            <td>31.24</td>
            <td>20.71</td>
            <td>10.53</td>
            <td>41.15</td>
          </tr>
          <tr>
            <td style="text-align: left;">CLIP (ViT-B/16)</td>
            <td>86.53</td>
            <td>79.1</td>
            <td>71.68</td>
            <td>70.75</td>
            <td>65.31</td>
            <td>52.38</td>
            <td>8.84</td>
            <td>3.22</td>
            <td>0.8</td>
            <td>34.3</td>
            <td>22.58</td>
            <td>13.58</td>
            <td>42.42</td>
          </tr>
          <tr>
            <td style="text-align: left;">SigLip (ViT-B/16)</td>
            <td>90.5</td>
            <td>88.95</td>
            <td>83.25</td>
            <td>86.05</td>
            <td>79.25</td>
            <td>73.13</td>
            <td>11.58</td>
            <td>6.43</td>
            <td>1.77</td>
            <td>60.95</td>
            <td>47.03</td>
            <td>38.37</td>
            <td>55.61</td>
          </tr>
          <tr>
            <td style="text-align: left;">SigLip (ViT-L/16)</td>
            <td>93.44</td>
            <td>88.43</td>
            <td>84.46</td>
            <td>84.35</td>
            <td>78.23</td>
            <td>68.37</td>
            <td>10.29</td>
            <td>4.82</td>
            <td>1.29</td>
            <td>61.8</td>
            <td>57.05</td>
            <td class="highlight">44.14</td>
            <td class="highlight">56.39</td>
          </tr>

          <tr>
            <td style="text-align: left;">BLIP</td>
            <td>92.4</td>
            <td>92.57</td>
            <td class="highlight">87.05</td>
            <td>88.44</td>
            <td>86.73</td>
            <td class="highlight">78.57</td>
            <td>11.25</td>
            <td>4.98</td>
            <td class="highlight">2.09</td>
            <td>52.97</td>
            <td>46.01</td>
            <td>33.28</td>
            <td>56.36</td>
          </tr>
          <tr>
            <td style="text-align: left;">Coca</td>
            <td>84.97</td>
            <td>81.52</td>
            <td>73.58</td>
            <td>78.57</td>
            <td>66.33</td>
            <td>57.82</td>
            <td>11.25</td>
            <td>5.95</td>
            <td>1.77</td>
            <td>60.1</td>
            <td>35.82</td>
            <td>28.52</td>
            <td>48.85</td>
          </tr>


          <tr class="family-header">
            <td colspan="14">MLLMs</td>
          </tr>

          <tr>
            <td style="text-align: left;">LlaVa1.6 (7B)</td>
            <td>93.0</td>
            <td>32.8</td>
            <td>32.2</td>
            <td>92.2</td>
            <td>34.4</td>
            <td>33.3</td>
            <td>91.8</td>
            <td>7.8</td>
            <td>7.4</td>
            <td>73.6</td>
            <td>25.0</td>
            <td>20.2</td>
            <td>38.28</td>
          </tr>
          <tr>
            <td style="text-align: left;">Idefics2</td>
            <td>95.4</td>
            <td>69.4</td>
            <td class="highlight">67.6</td>
            <td>89.1</td>
            <td>71.4</td>
            <td class="highlight">67.0</td>
            <td>18.6</td>
            <td>18.8</td>
            <td>4.8</td>
            <td>72.2</td>
            <td>50.6</td>
            <td class="highlight">47.0</td>
            <td class="highlight">55.99</td>
          </tr>
          <tr>
            <td style="text-align: left;">InternVL1.5</td>
            <td>94.65</td>
            <td>40.24</td>
            <td>39.72</td>
            <td>91.16</td>
            <td>42.86</td>
            <td>41.16</td>
            <td>74.28</td>
            <td>14.79</td>
            <td class="highlight">11.74</td>
            <td>73.51</td>
            <td>31.58</td>
            <td>27.5</td>
            <td>48.60</td>
          </tr>

          <tr class="family-header">
            <td colspan="14">Enterprise APIs</td>
          </tr>

          <tr>
            <td style="text-align: left;">GPT4V Turbo</td>
            <td>95.06</td>
            <td>93.58</td>
            <td class="highlight">91.11</td>
            <td>90.48</td>
            <td>88.44</td>
            <td class="highlight">82.65</td>
            <td>63.37</td>
            <td>28.71</td>
            <td class="highlight">20.79</td>
            <td>68.46</td>
            <td>53.41</td>
            <td class="highlight">43.37</td>
            <td class="highlight">68.29</td>
          </tr>
          <tr>
            <td style="text-align: left;">Gemini1.5 Pro</td>
            <td>94.8</td>
            <td>79.21</td>
            <td>78.22</td>
            <td>91.84</td>
            <td>74.83</td>
            <td>72.45</td>
            <td>56.94</td>
            <td>15.97</td>
            <td>9.03</td>
            <td>70.3</td>
            <td>48.51</td>
            <td>40.59</td>
            <td>61.05</td>
          </tr>

          <!-- Add other rows as needed and highlight the best scores in each category -->
        </tbody>
      </table>
    </div>

  <!-- Key Observations -->
  <div class="content">
    <p>
      Our analysis revealed that <strong>text tasks generally outperform image-based tasks</strong>, indicating alignment challenges between images and captions. In <strong>object and attribute recognition</strong>, models demonstrated strong capabilities, with foundational Visual Language Models (VLMs) often outperforming Multimodal Large Language Models (MLLMs) in image-related tasks. However, both types of models faced significant challenges in <strong>spatial reasoning</strong>, highlighting a crucial area for future advancements. Notably, <strong>human participants outperformed models</strong> in complex scene comprehension, suggesting areas for model improvement.
    </p>
  </div>
  <!-- End Key Observations -->
  </div>
</section>
<!-- End VisMin Results -->


<!-- Performance of Fine-Tuned CLIP and Idefics2 -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Performance of Fine-Tuned CLIP and Idefics2 </h2>
    <!-- <h2 class="title is-4">Performance on VisMin Benchmark</h2> -->
    
    <article class="message is-info">
      <div class="message-body">
        <strong>Setup.</strong> We fine-tuned pretrained CLIP on our synthetic data, naming it Visual Minimal Enhanced CLIP (VisMin-CLIP), and compared its performance with NegCLIP, which trains the same CLIP using nearest neighbor images with associated captions as hard negatives and generates additional false captions through random word swaps. Both models use ViT-L/14 as their backbone and the original CLIP loss, initialized from OpenAI checkpoints.
      </div>
    </article>

    <div class="table-wrapper mb-5">
      <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
        <caption>Performance comparison of VisMin variants of base CLIP and Idefics2. The table demonstrates how these models perform in terms of different categories, highlighting the effectiveness of our training set in improving fine-grained understanding in VLMs.</caption>
        
        <thead class="center">
          <tr class="oddrow">
            <th></th>
            <th colspan="3">Object</th>
            <th colspan="3">Attribute</th>
            <th colspan="3">S. Relation</th>
            <th colspan="3">Count</th>
            <th>AVG</th>
          </tr>
          <tr>
            <th></th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th></th>
          </tr>
        </thead>
        <tbody class="center">
          <tr class="oddrow">
            <td style="text-align: left;">CLIP </td>
            <td>87.56</td>
            <td>83.59</td>
            <td>78.07</td>
            <td>74.49</td>
            <td>69.73</td>
            <td>57.82</td>
            <td>9.16</td>
            <td>4.66</td>
            <td class="highlight">1.45</td>
            <td>37.01</td>
            <td>30.56</td>
            <td>18.17</td>
            <td>46.02</td>
          </tr>
          <tr>
            <td style="text-align: left;">VisMin-CLIP</td>
            <td>91.54</td>
            <td>91.19</td>
            <td class="highlight">86.36</td>
            <td>85.03</td>
            <td>83.67</td>
            <td class="highlight">75.85</td>
            <td>11.9</td>
            <td>3.38</td>
            <td>1.29</td>
            <td>82.34</td>
            <td>79.97</td>
            <td class="highlight">72.33</td>
            <td class="highlight">63.74</td>
          </tr>

          <tr>
            <td style="text-align: left;">Idefics2</td>
            <td>95.4</td>
            <td>69.4</td>
            <td>67.6</td>
            <td>89.1</td>
            <td>71.4</td>
            <td>67.0</td>
            <td>18.6</td>
            <td>18.8</td>
            <td>4.8</td>
            <td>72.2</td>
            <td>50.6</td>
            <td>47.0</td>
            <td>55.99</td>
          </tr>
          <tr>
            <td style="text-align: left;">Idefics2-VisMin</td>
            <td>96.5</td>
            <td>95.7</td>
            <td class="highlight">93.3</td>
            <td>91.2</td>
            <td>91.8</td>
            <td class="highlight">86.7</td>
            <td>83.0</td>
            <td>76.0</td>
            <td class="highlight">69.3</td>
            <td>85.4</td>
            <td>87.8</td>
            <td class="highlight">80.5</td>
            <td class="highlight">86.43</td>
          </tr>

        </tbody>
      </table>
    </div>

    <!-- <h2 class="title is-4">Evaluation on Fine-Grained Understanding OOD Benchmarks</h2> -->
    <div class="table-wrapper mb-5">
      <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
        <caption>Evaluation on Fine-Grained Understanding of <strong>Out-of-Distribution (OOD)</strong> Benchmarks. All models utilize ViT-L-14 as the vision encoder. The acronym 'CB' refers to CountBench, with 'SG' to SugarCrepe, and 'IC' to ImageCode. Our training set shows consistent improvements over the baseline model across OOD benchmarks. Additionally, we demonstrate that models employing minimally-changed images outperform NegCLIP, which utilizes nearest neighbor images.</caption>
        <thead class="center">
          <tr>
            <th></th>
            <th colspan="4">Single-Image</th>
            <th colspan="11">Multi-Image</th>
            <th>AVG</th>
          </tr>
          <tr>
            <th></th>
            <th>VSR</th>
            <th>CB</th>
            <th>Valse</th>
            <th>SG</th>
            <th>Whatsup</th>
            <th colspan="2">SPEC</th>
            <th>IC</th>
            <th>MMVP</th>
            <th colspan="3">Winoground</th>
            <th colspan="3">EQBEN</th>
            <th></th>
          </tr>
          <tr>
            <th></th>
            <th></th>
            <th></th>
            <th></th>
            <th></th>
            <th></th>
            <th>I2T</th>
            <th>T2I</th>
            <th></th>
            <th></th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th>T</th>
            <th>I</th>
            <th>G</th>
            <th></th>
          </tr>
        </thead>
        <tbody class="center">
          <tr>
            <td style="text-align: left;">CLIP (ViT-L/14)</td>
            <td>58.33</td>
            <td>33.65</td>
            <td>69.1</td>
            <td>73.0</td>
            <td>37.7</td>
            <td>32.85</td>
            <td>30.86</td>
            <td>61.47</td>
            <td>19.26</td>
            <td>27.5</td>
            <td>11.0</td>
            <td>8.5</td>
            <td>35.71</td>
            <td>33.57</td>
            <td>21.43</td>
            <td>36.93</td>
          </tr>
          <tr>
            <td style="text-align: left;">NegCLIP</td>
            <td>56.56</td>
            <td>40.0</td>
            <td class="highlight">75.41</td>
            <td class="highlight">85.73</td>
            <td>41.2</td>
            <td>37.73</td>
            <td>35.45</td>
            <td class="highlight">67.33</td>
            <td>29.63</td>
            <td>25.25</td>
            <td>12.0</td>
            <td>7.0</td>
            <td>42.86</td>
            <td>40.0</td>
            <td>30.0</td>
            <td>41.74</td>
          </tr>
          <tr>
            <td style="text-align: left;">VisMin-CLIP</td>
            <td class="highlight">58.69</td>
            <td class="highlight">49.84</td>
            <td>72.24</td>
            <td>81.44</td>
            <td class="highlight">43.99</td>
            <td class="highlight">44.28</td>
            <td class="highlight">39.98</td>
            <td>66.81</td>
            <td class="highlight">32.59</td>
            <td class="highlight">32.75</td>
            <td class="highlight">14.75</td>
            <td class="highlight">9.75</td>
            <td class="highlight">54.29</td>
            <td class="highlight">40.71</td>
            <td class="highlight">33.57</td>
            <td class="highlight">45.05</td>
          </tr>
          <tr>
            <td style="text-align: left;">Idefics2</td>
            <td>77.3</td>
            <td>91.11</td>
            <td class="highlight">88.91</td>
            <td>90.45</td>
            <td>68.04</td>
            <td>74.37</td>
            <td>60.5</td>
            <td>64.4</td>
            <td>48.15</td>
            <td class="highlight">47.25</td>
            <td>33.75</td>
            <td>22.5</td>
            <td>62.88</td>
            <td>33.33</td>
            <td>25.76</td>
            <td>59.25</td>
          </tr>
          <tr>
            <td style="text-align: left;">Idefics2-Vismin</td>
            <td class="highlight">80.32</td>
            <td class="highlight">93.97</td>
            <td>86.08</td>
            <td class="highlight">91.14</td>
            <td class="highlight">74.42</td>
            <td class="highlight">76.2</td>
            <td class="highlight">76.48</td>
            <td class="highlight">70.7</td>
            <td class="highlight">48.89</td>
            <td>47.0</td>
            <td class="highlight">35.75</td>
            <td class="highlight">22.5</td>
            <td class="highlight">64.39</td>
            <td class="highlight">54.55</td>
            <td class="highlight">49.24</td>
            <td class="highlight">64.78</td>
          </tr>

          
        </tbody>
      </table>
    </div>
  </div>

  <div class="container is-max-desktop">
    <p><strong>Observation #1: Enhanced Model Performance through Fine-Tuning.</strong> The fine-tuning of the CLIP model using the Visual Minimal Enhanced CLIP (VisMin-CLIP) approach, which involves training on synthetic data with minimal changes, significantly enhances its performance in the Object, Attribute, and Count categories. This improvement demonstrates the effectiveness of using tailored, fine-grained synthetic data to boost the model's understanding in specific visual recognition tasks.</p>

    <p><strong>Observation #2: Challenges in Spatial Relation Understanding.</strong> Despite improvements in several categories, VisMin-CLIP exhibits a slight degradation in the Spatial Relations category. This indicates inherent difficulties that the CLIP model faces in mastering spatial relationships, suggesting a potential area for further research to enhance spatial reasoning in visual language models.</p>

    <p><strong>Observation #3: Comparative Benefits of Data Augmentation.</strong> The comparison between VisMin-CLIP and NegCLIP highlights the advantages of specific data augmentation strategies. While NegCLIP, which uses hard negatives and linguistic variations, shows benefits in single-image text retrieval tasks, VisMin-CLIP's focused synthetic data approach yields more substantial improvements in tasks requiring detailed understanding of counting and spatial relationships, underlining the importance of tailored augmentation methods for complex cognitive tasks.</p>
  </div>

  <div class="container is-max-desktop mt-6 mb-5">
    <h3 class="title is-4">Additional Findings</h3>
  </div>

  <div class="container">
    <div class="columns">
      <div class="column">
        <figure class="image mb-5">
          <img src="static/images/scatter_plot_horizontal.jpg" alt="Scaling with VisMin data">
        </figure>
      </div>
      <div class="column">
        <figure class="image mb-5">
          <img src="static/images/combined_plots.jpg" alt="Scaling with VisMin data" style="width: 85%;">
        </figure>
      </div>
    </div>
  </div>
  
  <div class="container is-max-desktop">
  <p>Further experiments reveal as shown in the figures above. <strong>Scalability:</strong> Larger CLIP models like B/32 and L/16 perform better on our synthetic data, especially on complex tasks like understanding minimal changes. For example, improvements increased from 2.37 to 6.88 in single image benchmarks when model capacity was expanded from ViT-B/32 to ViT-L/14. <strong>Enhanced Original Capabilities:</strong> Training on our data also improves performance in standard retrieval tasks, suggesting better alignment from training on minimal change tasks. This indicates our data's applicability across various cross-modal tasks.</p>
  </div>
</section>

<!-- End Performance of Fine-Tuned CLIP and Idefics2 --> 


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<div class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Conclusions and Limitations</h2>

    <p><strong>Introduced Challenging Benchmark:</strong> We introduced VisMin, a benchmark for evaluating fine-grained visual understanding in VLMs such as CLIP, SigLIP, LLaVA, and Idefics2. While these models excel at object recognition, they struggle with counting and spatial relationships.</p>
    <p><strong>Addressing Challenges via Fine-tuning:</strong> To address these challenges, we finetuned CLIP and Idefics2 on our minimal-change dataset, which led to significant improvements in object recognition, attributes, and counting. However, spatial relations remain a challenge, with models like GPT4V performing barely above random chance.</p>
    <p><strong>Training Data for Building VLMs:</strong> Despite some noise in the training data due to limitations of current diffusion models, our dataset shows potential as a robust training resource for VLMs.</p>
    <p><strong>Limitations:</strong> Our experiments used uniform, simple prompts for consistent evaluation, which may have variably influenced model performance.</p>
  </div>
</div>
<!-- End of Conclusion-->

<!-- Previous Work Section -->
<section class="section hero is-featured">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="color: white;">
      Our Relevant Work at CVPR2024
    </h2>
    
    <div class="columns">
      <div class="column is-one-quarter-desktop is-full-mobile">
     
        <div style="display: flex; align-items: center; justify-content: space-between;">
          <figure class="image is-inline-block">
            <img src="static/images/cvpr-navbar-logo.svg" alt="CVPR2024 Logo" style="height: 3em; vertical-align: middle;">
          </figure>
          <figure class="image is-inline-block">
            <img src="static/images/milalogowebblancrgb.png" alt="Mila Logo" style="height: 3em; vertical-align: middle;">
          </figure>
        </div>
        <figure class="image">
          <img src="static/images/le_intra_cross_cvpr2024.png" alt="Key figure from previous research" style="height: auto; max-height: 240px;">
        </figure>
        <!-- Button links directly below image -->
        <div class="buttons has-addons mt-2 mb-3"> <!-- Adjust margin top and bottom -->
          <a href="https://arxiv.org/abs/2306.08832" target="_blank" class="button is-small is-link">ArXiv</a>
          <a href="https://github.com/lezhang7/Enhance-FineGrained" target="_blank" class="button is-small is-info">GitHub</a>
        </div>
      </div>
      <div class="column is-three-quarters-desktop is-full-mobile">
        <h3 class="title is-4">
          <a href="https://arxiv.org/abs/2306.08832" target="_blank" style="color: white; text-decoration: none;">
            Contrasting Intra-modal and Ranking Cross-modal Hard Negatives to Enhance Visio-linguistic Compositional Understanding
          </a>
        </h3>
        <div class="is-size-5 publication-authors">
          <!-- Paper authors -->
          <span class="author-block">
            <a href="https://zhangle.netlify.app/" target="_blank">Le Zhang</a>,
          </span>
          <span class="author-block">
            <a href="https://rabiul.me/" target="_blank">Rabiul Awal</a>,
          </span>
          <span class="author-block">
            <a href="https://www.iro.umontreal.ca/~agrawal/" target="_blank">Aishwarya Agrawal</a>
          </span>
        </div>
        <p class="content has-text-justified">
          Our current findings build upon our previous work, focusing on addressing the gaps in compositional reasoning by enhancing the alignment between images and captions. This research was pivotal in shaping the methodologies employed in our current project.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End of Previous Work Section -->



<!-- BibTeX citation -->
<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <p>If you found this work useful in your own research, please consider citing the following:</p>
      <pre><code>
        @article{zhang2023contrasting,
          title={Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding},
          author={Zhang, Le and Awal, Rabiul and Agrawal, Aishwarya},
          journal={arXiv preprint arXiv:2306.08832},
          year={2023}
        }
      </code></pre>
  </div>
</section>
<!-- End BibTeX citation -->




  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
